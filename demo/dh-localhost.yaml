
# This is a bit ugly, but, for now, we're going to define the entire cluster in this one file.
# This includes pulling out all the envoy config, so we can change things as we please
# without interfering with the simple docker-compose deployment using envoy.yaml in source tree
apiVersion: v1
kind: Secret
metadata:
  name: secret-tls
type: kubernetes.io/tls
data:
  # these are our *.localhost.int.illumon.com certs, and should NOT be committed as-is.
  # we'll want to instead make a parameterized
  tls.crt: >-
    SNIP
  tls.key: >-
    SNIP
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: envoy-conf
data:
  envoy.yaml: |
    admin:
      # access_log_path: /dev/stdout
      access_log_path: /tmp/admin_access.log
      address:
        socket_address:
          address: 127.0.0.1
          port_value: 9090
    static_resources:
      listeners:
      - name: listener_0
        address:
          socket_address:
            address: 0.0.0.0
            port_value: 10000
        filter_chains:
          - filters:
              - name: envoy.filters.network.http_connection_manager
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                  access_log:
                    - name: envoy.access_loggers.file
                      typed_config:
                        "@type": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog
                        path: "/dev/stdout" # easier debugging
                  codec_type: AUTO
                  stat_prefix: ingress_https
                  upgrade_configs:
                   - upgrade_type: websocket
                  route_config:
                    name: local_route
                    virtual_hosts:
                        - name: reverse_proxy
                          domains: ["*"]
                          routes:
                            - match: # Call to / goes to the landing page
                                path: "/"
                              route: { cluster: web }
                            - match: # Web IDE lives in this path
                                prefix: "/ide"
                              route: { cluster: web }
                            - match: # JS API lives in this path
                                prefix: "/jsapi"
                              route: { cluster: web }
                            - match: # Notebook file storage at this path
                                prefix: "/notebooks"
                              route: { cluster: web }
                            - match: # Any GRPC call is assumed to be forwarded to the real service
                                prefix: "/"
                              route:
                                cluster: server
                                max_stream_duration:
                                  grpc_timeout_header_max: 0s
                                timeout: 0s
                  http_filters:
                    - name: envoy.filters.http.health_check
                      typed_config:
                        "@type": type.googleapis.com/envoy.extensions.filters.http.health_check.v3.HealthCheck
                        pass_through_mode: false
                        headers:
                          - name: ":path"
                            exact_match: "/healthz"
                          - name: "x-envoy-livenessprobe"
                            exact_match: "healthz"
                    - name: envoy.filters.http.grpc_web
                    - name: envoy.filters.http.router
            transport_socket:
              name: envoy.transport_sockets.tls
              typed_config:
                "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext
                common_tls_context:
                  alpn_protocols: ["h2"]
                  tls_certificates:
                    - certificate_chain:
                        filename: /etc/ssl/tls.crt
                      private_key:
                        filename: /etc/ssl/tls.key
      clusters:
        - name: server
          connect_timeout: 10s
          type: LOGICAL_DNS
          lb_policy: ROUND_ROBIN
          http2_protocol_options: {}
          load_assignment:
            cluster_name: grpc-proxy
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          # address: server # here we assume the name of the websocket proxy
                          address: 127.0.0.1
                          port_value: 8080
        #      health_checks:
        #        timeout: 1s
        #        interval: 10s
        #        unhealthy_threshold: 2
        #        healthy_threshold: 2
        #        grpc_health_check: { }
        - name: web
          connect_timeout: 10s
          type: LOGICAL_DNS
          lb_policy: ROUND_ROBIN
          http_protocol_options: {}
          load_assignment:
            cluster_name: web
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      hostname: web
                      address:
                        socket_address:
                          # address: web
                          address: 127.0.0.1
                          port_value: 80
---
apiVersion: v1
kind: Service
metadata:
  name: dh-local
  labels:
    run: dh-local
spec:
  # NodePort allows you to expose a port on, say, localhost minikube ip: `minikube service --url dh-local`
  type: NodePort
  # If you want to run localhost with tls, you may use one of two ways,
  # assuming tls_port=8443
  # 1) create a self signed certificate using any DNS name you would like, and accept security warnings in browser:
  #    TODO: finish these instructions
  # 2) use an existing cert for your domain.name
  #    If you do not want to hack /etc/hosts or resolv.conf, pick a domain name and create a DNS A record pointing to 127.0.0.1
  #    If using DNS to 127.0.0.1, port-forward from minikube to localhost:
  #    kubectl port-forward pods/dh-local-6674d78b6-cc64l ${tls_port:-8443}:10000
  #    Next, supply your certificates (directory is .gitignore'd)
  #    cp tls.crt tls.key $deephaven_core_directory/demo/certs/
  #    Then, forcibly restart your local cluster
  #    { k delete deployment dh-local || true ; } && k apply -f ./pod-dh.yaml
  #    Now, visit https://domain.name:8433 and rejoice.
  ports:
    -
      name: envoy-client
      port: 10000
      targetPort: 10000
      protocol: TCP
      nodePort: 30080
    -
      name: envoy-admin
      port: 9090
      targetPort: 9090
      protocol: TCP
      nodePort: 30443
  selector:
    # This service runs a deployment. That deployment is dh-local, below:
    run: dh-local
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dh-local
spec:
  selector:
    matchLabels:
      run: dh-local
  replicas: 1
  template:
    metadata:
      labels:
        run: dh-local
    spec:
      volumes:
        - name: envoy-conf
          configMap:
            # Provide the name of the ConfigMap containing the files you want
            # to add to the container
            name: envoy-conf
        - name: envoy-secrets
          secret:
            secretName: secret-tls
        - name: cred-cache
          emptyDir:
            medium: Memory

      containers:

        #
        # server container.  Where all the stateful brains live
        #
        - name: server
          image: deephaven/server:local-build
          imagePullPolicy: Never
          securityContext:
            privileged: false
          # Change tty to true if you want to be able to shell into the box
          tty: true
          env:
          - name: CONTAINER_NAME
            value: server
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: MY_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                fieldPath: spec.serviceAccountName
          volumeMounts:
          - name: cred-cache
            mountPath: /root/.ssh
          readinessProbe:
            exec:
              command: [ "/health/grpc_health_probe", "-addr=:8080" ]
            initialDelaySeconds: 2
            periodSeconds: 1
            failureThreshold: 28
          livenessProbe:
            exec:
              command: [ "/health/grpc_health_probe", "-addr=:8080" ]
            initialDelaySeconds: 10
            periodSeconds: 8

        #
        # grpc-proxy: turns http1 grpc web requests into http2 fit for server to consume
        #
        - name: grpc-proxy
          image: deephaven/grpc-proxy:local-build
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
          # Change tty to true if you want to attach to the image entry point.
          # you can still launch a tty shell with `kubectl -c grpc-proxy -it MY_POD_NAME_HERE -- bash`
          # You can find MY_POD_NAME_HERE from `minikube dashboard` or, to find the newest pod containing name dh-local:
          # fmt="jsonpath={range .items[*]}{.status.startTime}{'\t'}{.metadata.name}{'\n'}{end}"
          # my_pod="$(kubectl get pods --sort-by=.metadata.creationTimestamp --no-headers -o="$fmt" | grep dh-local | tail -n 1 | awk '{print $2}')"
          # kubectl exec -c grpc-proxy -it $my_pod -- sh
          tty: false
      #    resources:
      #      # See: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md
      #      limits:
      #        # 4 cpus
      #        cpu: '4000m'
      #        # ~8G RAM
      #        memory: '8000Mi'
          env:
          - name: BACKEND_ADDR
            value: "localhost:8080"
          - name: CONTAINER_NAME
            value: grpc-proxy
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: MY_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                fieldPath: spec.serviceAccountName
          volumeMounts:
          - name: cred-cache
            mountPath: /root/.ssh


        #
        # web: serves the web ide and all static resources
        #
        - name: web
          image: deephaven/web:local-build
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
          # Change tty to true if you want to be able to shell into the box
          tty: false
          env:
          - name: CONTAINER_NAME
            value: web
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: MY_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                fieldPath: spec.serviceAccountName
          volumeMounts:
          - name: cred-cache
            mountPath: /root/.ssh


        #
        # envoy: this should really be a stateful set / deployment instead of a container-within-this-pod.
        # As a stateful set, the container would be added automatically to all nodes, plus it would have stable addressing.
        #
        - name: envoy
          image: envoyproxy/envoy:v1.18.3
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
          # Change tty to true if you want to be able to attach directly to the envoy process
          tty: false
          ports:
            - name: https
              containerPort: 8443
            - name: http
              containerPort: 8181
              protocol: TCP
              # haven't figured out readiness probe yet
#          readinessProbe:
#            httpGet:
#              port: https
#              httpHeaders:
#                - name: x-envoy-livenessprobe
#                  value: healthz
#              path: /healthz
#              scheme: HTTPS
          env:
          - name: CONTAINER_NAME
            value: envoy
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: MY_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                fieldPath: spec.serviceAccountName
          volumeMounts:
          - name: cred-cache
            mountPath: /root/.ssh
          - name: envoy-conf
            mountPath: /etc/envoy
          - name: envoy-secrets
            mountPath: /etc/ssl
            readOnly: true